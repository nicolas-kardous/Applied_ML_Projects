{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4, due March 16 at 11:59pm\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions (especially the sections on vectorized computation and computational efficiency).\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
    "* Part 4 (especially 4.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>15.534711</td>\n",
       "      <td>397.462329</td>\n",
       "      <td>5.715647</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>17.914131</td>\n",
       "      <td>397.012611</td>\n",
       "      <td>9.338417</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>17.919989</td>\n",
       "      <td>396.628236</td>\n",
       "      <td>4.142473</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.979527</td>\n",
       "      <td>398.564784</td>\n",
       "      <td>3.239272</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>18.708888</td>\n",
       "      <td>399.487766</td>\n",
       "      <td>6.115159</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX    PTRATIO           B     LSTAT  MEDV  \n",
       "0  307.0  15.534711  397.462329  5.715647  24.0  \n",
       "1  255.0  17.914131  397.012611  9.338417  21.6  \n",
       "2  243.0  17.919989  396.628236  4.142473  34.7  \n",
       "3  226.0  18.979527  398.564784  3.239272  33.4  \n",
       "4  234.0  18.708888  399.487766  6.115159  36.2  "
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load you data the Boston Housing data into a dataframe\n",
    "# MEDV.txt containt the median house values and data.txt the other 13 features\n",
    "# in order [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n",
    "\n",
    "datatxt = np.loadtxt('data.txt')\n",
    "MEDVtxt = np.loadtxt('MEDV.txt')\n",
    "\n",
    "data = pd.DataFrame(datatxt, columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"])\n",
    "data['MEDV'] = MEDVtxt\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between  median housing price and number of rooms per house. Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, observed):\n",
    "    rmse = np.sqrt(np.sum((predictions - observed)**2)/len(predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Cost for room = 6.67\n",
      "Mean Squared Error Cost for room and room squared = 6.24\n",
      "Linear Model Coefficients = 8.96\n",
      "Quadratic Model Coefficients = -23.79,2.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## (a) regressing the housing price on the number of rooms per house\n",
    "\n",
    "rm = data[['RM']]\n",
    "y = data['MEDV']\n",
    "LR1 = LinearRegression().fit(rm, y)\n",
    "pred1 = LR1.predict(rm)\n",
    "cost1 = compute_rmse(pred1, y)\n",
    "\n",
    "## (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.\n",
    "\n",
    "rm2 = rm.copy()\n",
    "rm2['RM2'] = rm**2\n",
    "LR2 = LinearRegression().fit(rm2, y)\n",
    "pred2 = LR2.predict(rm2)\n",
    "cost2 = compute_rmse(pred2, y)\n",
    "\n",
    "plt.plot(rm ,pred1, c='C0', label='linear')\n",
    "plt.scatter(rm ,pred2, c='C2', label='quadratic')\n",
    "plt.scatter(rm, y, c = 'C3',label='observed')\n",
    "plt.title(\"Predictions on median housing prices from number of rooms per house\")\n",
    "plt.xlabel(\"Average number of rooms per dwelling\")\n",
    "plt.ylabel(\"Median value of homes in $1000's\")\n",
    "plt.legend()\n",
    "\n",
    "print('Mean Squared Error Cost for room = {:.2f}'.format(cost1))\n",
    "print('Mean Squared Error Cost for room and room squared = {:.2f}'.format(cost2))\n",
    "print('Linear Model Coefficients = {:.2f}'.format(LR1.coef_[0]))\n",
    "print('Quadratic Model Coefficients = {:.2f},{:.2f}'.format(LR2.coef_[0],LR2.coef_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, we see that our linear model and quadratic model both follows the trend of the data pretty well. Some issues we see is that there are some outliers where the data points are scattered around the graph. From our mean squared error costs, we see that the model that includes rm and rm^2 features has a lowest cost compared to the model that only uses the rm feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 25-fold cross-validation to fit regression (a) above, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the 25 slope coefficients using a histogram, then draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope coefficients for each kfold \n",
      " = [8.94682556 8.89463907 8.9603567  8.98419985 8.94375822 8.99403578\n",
      " 8.94901254 8.79793532 8.86389089 8.42086688 8.96647507 8.83902895\n",
      " 8.65941046 8.51651748 8.89746381 9.00426798 8.97467577 9.61858367\n",
      " 9.74689177 8.74894341 9.1583665  9.00180018 9.08203655 8.9746311\n",
      " 8.99684028]\n",
      "Average of slope coefficients in 1.2 = 8.957658151090296\n",
      "value of slope coefficient in 1.1 = 8.959927212241066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=25)\n",
    "y = data[['MEDV']]\n",
    "\n",
    "train_index = []\n",
    "test_index = []\n",
    "for train, test in kf.split(rm):\n",
    "    train_index.append(train)\n",
    "    test_index.append(test)\n",
    "\n",
    "coefficients = np.zeros(25)\n",
    "for i in range(25):\n",
    "    LR = LinearRegression().fit(rm.iloc[train_index[i],:], y.iloc[train_index[i],:])\n",
    "    coefficients[i] = LR.coef_[0]\n",
    "\n",
    "    \n",
    "print('Slope coefficients for each kfold \\n = {}'.format(coefficients))  \n",
    "print('Average of slope coefficients in 1.2 = {}'.format(np.mean(coefficients)))\n",
    "print('value of slope coefficient in 1.1 = {}'.format(LR1.coef_[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c25c97a90>"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXwV5dn/8c8FYQcXIFoWNdCiKIshgiAoS0FtRaUo1iIq1IKl1qK2uLS2Vqm0+tOnUvTxQUoVRcVH0CIKrUJ9EIqIgMQFUEGJCGrZREB2vH5/zCQcQpZJSM7kHL7v1yuvnGXO3N+zXjP3zNxj7o6IiBzZqsUdQERE4qdiICIiKgYiIqJiICIiqBiIiAgqBiIigopBlWFmy8ysZ9w54mRm/c3sUzPbbmYdktTmKWa21My2mdkIM6tjZi+a2VdmNsXMBpnZKxHm8xszm5CMzIfDzPLMrE+S2/yZmf0nfF8bJbNtic50nEHlM7M8YKi7z064bUh429llmE8WsBqo4e77KjZl/MzsI+CX7v5CEtv8G7DV3W8Kr18F/ALoGsdrXNnvcVGfxcpkZjWArUAXd387GW0WkaEm8BtgENAU2AC8CowCfg3UcferCz2mPbAIaOLum5ObOB5aM5ACZpYRc4STgGUxt3kS8GE6FtuYHA/Uppj3NUmfuanAxcAVwNHA6cASoDcwEbjEzOoVeszVwEtHSiEAwN31V8l/QB7Qp9BtQ4B/FzUNcCawmGCJ6j/An8Pb1wAObA//ziIo6L8FPgHWA08ARyfM9+rwvk3A7wq1cyfBF+XJsK2hYdsLgC3A58BDQM2E+TlwHbAS2Ab8Afh2+JitwLOJ0xd6zkVmBWqFz8eBr4GPinl8G2AWsDl8XX4T3l4LGAN8Fv6NAWolPO5CIDd8Tq8D7cPbXwX2A7vC9icDe4C94fWfFPE+FZfhTuDJhOm6hG1tAd4GeibcNyd83eaHr+ErQOPi3uNCr0FTYCfQMOG2DsBGoEb4Xrwavt8bgaeAY4r5nE0E7k64ryewtlBbzxEsSa8GRiTcV+RntFDWk8P3M//5vJrwGfo5wWdodXhbV4Il8a/C/10LvV53h6/nduBFoFH43LaG02cV85npE75eJ5Tw/fwAuDrhevXwc3Rx3L8dyfyLPcCR8EfZi8EC4Krwcn2CVWyArPCLlJHwuGuAVUDLcNrngUnhfaeFX56zgZrA/QQ/dInFYC/wA4If6jrAGQQ/ZBlheyuAGxPac2A6cBTBD+Nu4F9h+0cDy4HBxbwOxWZNmPd3inlsA4Li9CuCJc0GQOfwvlHAG8BxQGb4o/GH8L4cgsLTOfySDw5f61rh/XMIuk1IeE2eLOp9KiVDweOAZgQ/xheEr+u54fXMhDY/IvixrBNev6e497iI1+JVYFjC9fuAceHl74Tt1Qpfi7nAmGI+ZxMpphiEuZcAdxB8dloCHwPnl/QZLSLrIc8nvD4LaBg+/4bAl8BVBJ+7geH1Rgmv1yqCQpf/GfuQ4Ic+g2Ch4rFi2r8HeK2U7+ftwOyE6+cTFMAacf92JPNP3UTJM83MtuT/AQ+XMO1e4Dtm1tjdt7v7GyVMO4hgqexjd99O0Af6o3D1ewDworv/2933EHyxC28kWuDu09z9G3ff6e5L3P0Nd9/n7nnAI0CPQo+51923uvsy4D3glbD9r4B/ECypljVraS4EvnD3/3L3Xe6+zd0XJsx3lLuvd/cNwF0EPywAw4BH3H2hu+9398cJCliXCG2WJUOiK4GZ7j4zfF1nESxFX5AwzWPu/qG77yRYm8ouQ46nCX4wMTMDfhTehruvcvdZ7r47fC3+zKHvXxSdCIrXKHff4+4fA38N24KyfUaL8id33xw+/77ASnefFH7uJgPvAxclTP+Yu3+U8Bn7yN1ne9CdN4XiP3ONCAp4SSYBPcyseXj9auBpd99bxueU0lQMkucH7n5M/h9BV0txfkKw1Pi+mS0yswtLmLYpQbdLvk8IlpaOD+/7NP8Od99BsISa6NPEK2Z2spm9ZGZfmNlW4I9A40KP+U/C5Z1FXK9fjqylOYFgaTrqfJuGl08CflWoEJ+QcH9ZlJQh0UnAZYXaPBtokjDNFwmXd1D8a1aUqcBZZtYU6E5Q4OcBmNlxZvaMma0L378nOfT9i+IkoGmh5/AbDrxXZfmMFiXxc1f4/SO83izhenk/c5s4+HU/hLuvIViDutLM6hOsKT9e0mPSkYpBFeTuK919IEG3x73A1HADV1G7fn1G8MXNdyKwj+DL8jmQv7SDmdUhWFI6qLlC1/+HYKmslbsfRfADYOV/NpGzluZTgm6CqPP9LOFxoxMLsbvXDZc+y6qkDIWnm1SozXrufk+Ex5a6e5+7byHYzvBDgo2ik909/3F/CufRPnz/rqT49+9roG7C9W8Veg6rCz2HBu5+QZihuM9oVInPs/D7B8F7uK4M8yvObODMhKX+4jxOsEZwKcHzfqsC2k4pKgZVkJldaWaZ7v4NwQZICDZ0bgC+Iei/zTcZuMnMWoRLNX8E/jdcfZ4KXGRmXcPd6+6i9B/2BgQb5babWWvgZxX2xErOWpqXgG+Z2Y1mVsvMGphZ54T5/tbMMs2sMUF32JPhfX8FhptZZwvUM7O+ZtagHPlLypDoSYLX/Xwzq25mtc2sZ4QfJCj6PS7K0xz48Xo64fYGBNuJtphZM+DmEuaRC1xgZg3N7FvAjQn3vQlsNbNbw2MvqptZWzPrBCV+RstjJnCymV1hZhlmdjnB9q6Xyjm/Ah7sQjsL+LuZnRHOv4GZDTezaxImfY5gze8ujsC1AlAxqKq+Bywzs+3AX4AfhX3UO4DRwPxw1b0L8ChBn+dcgj0+dhHsJ0/Yp/8L4BmCtYRtBBtTd5fQ9kiCpc1tBD+k/1uBz6vYrKVx920EG0YvIuhiWQn0Cu++m6BP/h3gXeCt8DbcfTHBdoOHCDZKriLYKFxmpWRInO5ToB/BWtUGgqXsm4nwfSvmPS7KdKAV8B8/eP/9uwg2mn8FzCDYSF+cSQR7OuURrGkUvNfuvj98ntkE79VGYALBBlwo5jNa2vMrirtvItge8yuCbp1bgAvdfWN55leEAQQF538JXpf3gI4Eaw35Gb7mQEF4qoLaTSk66OwIEi6NbyHoAloddx4RqTq0ZpDmzOwiM6sb9ufeT7DknBdvKhGpalQM0l8/DhyM1YpgdV6rgyJyEHUTiYiI1gxERCQ44KfKa9y4sWdlZcUdQ6q4PauDbeI1W7SIOYlI1bBkyZKN7p4ZZdqUKAZZWVksXrw47hhSxX1yVTAK8UmTnog5iUjVYGaFj+wulrqJREQkNdYMRKJo/LPhcUcQSVkqBpI26nXtGncEkZSlYiBpY9eKFQDUPvXUmJOkhr1797J27Vp27SrXKBJShdSuXZvmzZtTo0aNcs9DxUDSxn/++CdAG5CjWrt2LQ0aNCArK4vgtAiSitydTZs2sXbtWlocxp502oAscoTatWsXjRo1UiFIcWZGo0aNDnsNr9KKgZk9ambrzey9hNsamtksM1sZ/j+2stoXkdKpEKSHingfK3PNYCLBMLeJbgP+5e6tCM6be1slti8iIhFVWjFw97nA5kI39+PAiSMeJzi9nIhIlfH++++TnZ1Nhw4d+OijA2c53bFjB3379qV169a0adOG2247sCw7ceJEMjMzyc7OJjs7mwkTJhQ57w0bNtC5c2c6dOjAvHnzis2QlZXFxo2Hns7hzjvv5P777z+MZ1e8ZG9APt7dPwdw98/N7LjiJjSza4FrAU488cQkxZNUlXXbDE6tHZx0bMVtM5LWbt49fZPWliTHtGnT6NevH3fdddch940cOZJevXqxZ88eevfuzT/+8Q++//3vA3D55Zfz0EMPlTjvf/3rX7Ru3ZrHH696J1OrshuQ3X28u3d0946ZmZGG1pAj3IpGWaxolBV3DIkoLy+P1q1bM3ToUNq2bcugQYOYPXs23bp1o1WrVrz55psAfP3111xzzTV06tSJDh068MILLxQ8/pxzziEnJ4ecnBxef/11AObMmUPPnj0ZMGAArVu3ZtCgQRQ1OnNubi5dunShffv29O/fny+//JKZM2cyZswYJkyYQK9eB5/Erm7dugW31axZk5ycHNauXRv5+ebm5nLLLbcwc+ZMsrOz2blzJ5MnT6Zdu3a0bduWW2+9tcjHjR49mlNOOYU+ffrwwQcfRG6vrJK9ZvAfM2sSrhU0ITgFo0iFOHVTHoAKQjnlj+2UqMH3v0fDK67gm507+fTanx5y/9H9+3PMJf3Z9+WXrBtxw0H3RdnFd9WqVUyZMoXx48fTqVMnnn76af79738zffp0/vjHPzJt2jRGjx7Nd7/7XR599FG2bNnCmWeeSZ8+fTjuuOOYNWsWtWvXZuXKlQwcOLBgDLOlS5eybNkymjZtSrdu3Zg/fz5nn332QW1fffXVPPjgg/To0YM77riDu+66izFjxjB8+HDq16/PyJEji829ZcsWXnzxRW644cBzfu6555g7dy4nn3wyDzzwACeccMJBj8nOzmbUqFEsXryYhx56iM8++4xbb72VJUuWcOyxx3Leeecxbdo0fvCDA73nS5Ys4ZlnnmHp0qXs27ePnJwczjjjjFJf1/JI9prBdGBweHkw8EKS25c0NmT5TIYsnxl3DCmDFi1a0K5dO6pVq0abNm3o3bs3Zka7du3Iy8sD4JVXXuGee+4hOzubnj17smvXLtasWcPevXsZNmwY7dq147LLLmP58uUF8z3zzDNp3rw51apVIzs7u2Be+b766iu2bNlCjx49ABg8eDBz586NlHnfvn0MHDiQESNG0LJlSwAuuugi8vLyeOedd+jTpw+DBw8uZS6waNEievbsSWZmJhkZGQwaNOiQDPPmzaN///7UrVuXo446iosvvjhSxvKotDUDM5sM9AQam9la4PfAPcCzZvYTYA1wWWW1LyJlU9KSfLU6dUq8P+PYY8t1sF+tWrUOtFGtWsH1atWqsW/fPiA4qOq5557jlFNOOeixd955J8cffzxvv/0233zzDbVr1y5yvtWrVy+YV0W49tpradWqFTfeeGPBbY0aNSq4PGzYsIIun9tvv50ZM4JtWLm5uQfNJ+qJxZK1+29l7k000N2buHsNd2/u7n9z903u3tvdW4X/C+9tJCJykPPPP58HH3yw4Mdz6dKlQLB036RJE6pVq8akSZPYv39/5HkeffTRHHvssQV79EyaNKlgLaEkv/3tb/nqq68YM2bMQbd//vnnBZenT5/OqeGQKKNHjyY3N/eQQgDQuXNnXnvtNTZu3Mj+/fuZPHnyIRm6d+/O3//+d3bu3Mm2bdt48cUXIz/HstJwFCJSpf3ud7/jxhtvpH379rg7WVlZvPTSS1x33XVceumlTJkyhV69elGvXr0yzffxxx9n+PDh7Nixg5YtW/LYY4+VOP3atWsZPXo0rVu3JicnB4Drr7+eoUOHMnbsWKZPn05GRgYNGzZk4sSJpbbfpEkT/vSnP9GrVy/cnQsuuIB+/fodNE1OTg6XX3452dnZnHTSSZxzzjlleo5lkRLnQO7YsaPr5DZSkqzbZnDvvIcBuPWc65LWbirvWrpixYqCJVhJfUW9n2a2xN07Rnm81gwkbTzSrl/pE4lIkVQMJG18fEyzuCOIpKwqe9CZSFllr/+Q7PUfxh1DJCVpzUDSxsAPZgOQe9zJMScRST1aMxARERUDERFRN5GIhLIqeLTXZO12u3v3bvr27cvGjRv59a9/TdOmTRk+fDg1atRgxowZ3HDDDUydOrXYxw8dOpRf/vKXnHbaaWVue86cOdSsWZOuXbsezlOoElQMRCSlLV26lL179xYc5Tt8+HBGjhzJj3/8Y4ASCwFQ7LkHopgzZw7169dPi2KgbiJJG2OzBzA2e0DcMaSMnnjiCdq3b8/pp5/OVVddxSeffELv3r1p3749vXv3Zs2aNUBwYphLL72UTp060alTJ+bPn8/69eu58soryc3NJTs7m0ceeYRnn32WUaNGMWjQIPLy8mjbti0A+/fvZ+TIkbRr14727dvz4IMPAtCzZ8+C0U5feeUVzjrrLHJycrjsssvYvn07EJxs5ve//z05OTm0a9eO999/n7y8PMaNG8cDDzxAdnY28+bNY8qUKbRt25bTTz+d7t27x/Bqlp/WDCRtrGtQ7LmSpIpatmwZo0ePZv78+TRu3JjNmzczePBgrr76agYPHsyjjz7KiBEjmDZtGjfccAM33XQTZ599NmvWrOH8889nxYoVTJgwgfvvv5+XXnoJgAULFnDhhRcyYMCAg0YrHT9+PKtXr2bp0qVkZGSwefPBQ6Nt3LiRu+++m9mzZ1OvXj3uvfde/vznP3PHHXcA0LhxY9566y0efvhh7r//fiZMmHDIcNft2rXj5ZdfplmzZmzZsiU5L2IFUTGQtNH582UALGzSJuYkEtWrr77KgAEDaNy4MQANGzZkwYIFPP/88wBcddVV3HLLLQDMnj37oGGqt27dyrZt2yK3NXv2bIYPH05GRkZBW4neeOMNli9fTrdu3QDYs2cPZ511VsH9l1xyCQBnnHFGQb7CunXrxpAhQ/jhD39YMH2qUDGQtHHJqtcAFYNU4u6lDtGcf/8333zDggULqFOnTqW05e6ce+65TJ48ucj784fFLmlI7HHjxrFw4UJmzJhBdnY2ubm5Bw1vXZVpm4GIxKZ37948++yzbNq0CYDNmzfTtWtXnnnmGQCeeuqpgjOUnXfeeQedY7ioYaFLct555zFu3LiCH/LC3URdunRh/vz5rFq1CoAdO3bw4YclH9HeoEGDg9ZOPvroIzp37syoUaNo3Lgxn376aZkyxklrBiICxDMCa5s2bbj99tvp0aMH1atXp0OHDowdO5ZrrrmG++67j8zMzIKhpceOHcvPf/5z2rdvz759++jevTvjxo2L3NbQoUP58MMPad++PTVq1GDYsGFcf/31BfdnZmYyceJEBg4cyO7duwG4++67Ofnk4o9ov+iiixgwYAAvvPACDz74IA888AArV67E3enduzenn356OV+Z5NMQ1pIWNIR12WkI6/RyuENYq5tIRETUTSTp474zrog7gkjKUjGQtLGx7jFxR0g5UfbmkaqvIrr71U0kaaP72ly6ry3bHiZHstq1a7Np06YK+SGR+Lg7mzZtonbt2oc1H60ZSNrou/p1AOY2z445SWpo3rw5a9euZcOGDXFHkcNUu3ZtmjdvfljzUDEQOULVqFGDFi1axB1Dqgh1E4mIiIqBiIioGIiICNpmIGlk9JmD444gkrJUDCRtbK1VL+4IIilL3USSNvp8sog+nyyKO4ZISlIxkLRx7ppFnLtGxUCkPFQMRERExUBERGIqBmZ2k5ktM7P3zGyymR3eoBoiInJYkl4MzKwZMALo6O5tgerAj5KdQ0REDohr19IMoI6Z7QXqAp/FlEPSyB1nDY07gkjKSvqagbuvA+4H1gCfA1+5+yuFpzOza81ssZkt1qiKEsXujJrszqgZdwyRlBRHN9GxQD+gBdAUqGdmVxaezt3Hu3tHd++YmZmZ7JiSgvp+PJ++H8+PO4ZISopjA3IfYLW7b3D3vcDzQNcYckia6b7ubbqvezvuGCIpKY5isAboYmZ1LTjfXm9gRQw5REQkFMc2g4XAVOAt4N0ww/hk5xARkQNi2ZvI3X8P/D6OtkVE5FA6AllERDSEtaSPW8+5Lu4IIilLawYiIqJiIOnj0pVzuHTlnLhjiKQkFQNJG2d+sZwzv1gedwyRlKRiICIiKgYiIqJiICIiaNdSSSO7q9eIO4JIylIxkLRxR9dhcUcQSVnqJhIRERUDSR8D35/FwPdnxR1DJCWpGEjayN6wkuwNK+OOIZKSVAxERETFQEREtDeRVKCs22bEHUFEyknFQNLG1pr14o4gkrJUDCRtjO48OO4IIilL2wxERETFQNLHkGUzGbJsZtwxRFKSuokkbZy6OS/uCCIpS2sGIiKiYiAiIioGIiKCthlIGtlY55i4I4ikLBUDSRv3dbwi7ggiKUvdRCIiEq0YmFnbyg4icrh++s4L/PSdF+KOIZKSonYTjTOzmsBE4Gl331J5kUTKp+VX6+KOIJKyIq0ZuPvZwCDgBGCxmT1tZudWajIREUmayNsM3H0l8FvgVqAHMNbM3jezSyornIiIJEfUbQbtzewBYAXwXeAidz81vPxAJeYTEZEkiLrN4CHgr8Bv3H1n/o3u/pmZ/basjZrZMcAEoC3gwDXuvqCs8xFJtK5+ZtwRRFJW1GJwAbDT3fcDmFk1oLa773D3SeVo9y/AP919QLhhum455iFykLEdLos7gkjKirrNYDZQJ+F63fC2MjOzo4DuwN8A3H2P9k4SEYlX1GJQ2923518JL5d3ab4lsAF4zMyWmtkEM9P5CuWwjVg6hRFLp8QdQyQlRS0GX5tZTv4VMzsD2FnC9CXJAHKA/3H3DsDXwG2FJzKza81ssZkt3rBhQzmbkiNJs+0baLZdnxWR8oi6zeBGYIqZfRZebwJcXs421wJr3X1heH0qRRQDdx8PjAfo2LGjl7MtERGJIFIxcPdFZtYaOAUw4H1331ueBt39CzP71MxOcfcPgN7A8vLMS0REKkZZRi3tBGSFj+lgZrj7E+Vs9xfAU+GeRB8DPy7nfEREpAJEKgZmNgn4NpAL7A9vdqBcxcDdc4GO5XmsSHE+PrpZ3BFEUlbUNYOOwGnurr57qbIead8v7ggiKSvq3kTvAd+qzCAiIhKfqGsGjYHlZvYmsDv/Rne/uFJSiZTDzYufBnTGM5HyiFoM7qzMECIVofFOHcguUl5Rdy19zcxOAlq5+2wzqwtUr9xoIiKSLFGHsB5GcHDYI+FNzYBplRVKRESSK+oG5J8D3YCtUHCim+MqK5SIiCRX1G0Gu919j5kBYGYZBMcZiFQZKxpmxR1BJGVFLQavmdlvgDrhuY+vA16svFgiZTexzQVxRxBJWVG7iW4jGHb6XeCnwEyC8yGLiEgaiLo30TcEp738a+XGESm/2xc+DsDozoNjTiKSeqKOTbSaIrYRuHvLCk8kUk5H7fk67ggiKassYxPlqw1cBjSs+DgiIhKHSNsM3H1Twt86dx8DfLeSs4mISJJE7SbKSbhajWBNoUGlJBIRkaSL2k30XwmX9wF5wA8rPI3IYcjNbBV3BJGUFXVvol6VHUTkcE1ufW7cEURSVtRuol+WdL+7/7li4oiISBzKsjdRJ2B6eP0iYC7waWWEEimPUa8Hh8Hc0XVYzElEUk9ZTm6T4+7bAMzsTmCKuw+trGAiZVVr/964I4ikrKjDUZwI7Em4vgfIqvA0IiISi6hrBpOAN83s7wRHIvcHnqi0VCIiklRR9yYabWb/AM4Jb/qxuy+tvFgiIpJMUdcMAOoCW939MTPLNLMW7r66soKJlNWb3zot7ggiKSvqrqW/J9ij6BTgMaAG8CTB2c9EqoTnWvWMO4JIyoq6Abk/cDHwNYC7f4aGoxARSRtRi8Eed3fCYazNrF7lRRIpn3vnPcy98x6OO4ZISopaDJ41s0eAY8xsGDAbnehGRCRtRN2b6P7w3MdbCbYb3OHusyo1mYiIJE2pxcDMqgMvu3sfQAVARCQNldpN5O77gR1mdnQS8oiISAyiHmewC3jXzGYR7lEE4O4jKiWVSDnMbXZ63BFEUlbUYjAj/BOpsma01GEvIuVVYjEwsxPdfY27P17RDYfbIhYD69z9woqevxx5au0LxlLcnVEz5iQiqae0bQbT8i+Y2XMV3PYNwIoKnqccwUYtmMCoBRPijiGSkkorBpZwuWVFNWpmzYG+gL65IiJVQGnFwIu5fLjGALcA3xQ3gZlda2aLzWzxhg0bKrBpEREprLRicLqZbTWzbUD78PJWM9tmZlvL06CZXQisd/clJU3n7uPdvaO7d8zMzCxPUyIiElGJG5DdvXoltNkNuNjMLgBqA0eZ2ZPufmUltCUiIhGU5XwGFcLdfw38GsDMegIjVQikIsw6sVPcEURSVtKLgUhlmX2SioFIecVaDNx9DjAnzgySPo7aHRwcv7WWRlgXKauoQ1iLVHm3v/k4t79Z4cdHihwRVAxERETFQEREVAxERAQVAxERQbuWShqZ0aJr3BFEUpaKgaSNuc2z444gkrLUTSRpo/GOLTTesSXuGCIpScVA0sbNS57m5iVPxx1DJCWpGIiIiIqBiIioGIiICCoGIiKCdi2VNPL8d3rEHUEkZakYSNpY2KRN3BFEUpa6iSRtNNu2nmbb1scdQyQlqRhI2hiRO5URuVPjjiGSklQMRERExUBERFQMREQEFQMREUG7lkoamXxKn7gjiKQsFQNJG7nHnRx3BJGUpW4iSRstt6yj5ZZ1cccQSUkqBpI2fvruC/z03RfijiGSklQMRERExUBERFQMREQEFQMREUG7lkoamXjaBXFHEElZKgaSNlY0yoo7gkjKUjeRpI1TN+Vx6qa8uGOIpKSkFwMzO8HM/s/MVpjZMjO7IdkZJD0NWT6TIctnxh1DJCXF0U20D/iVu79lZg2AJWY2y92Xx5BFRESIYc3A3T9397fCy9uAFUCzZOcQEZEDYt2AbGZZQAdgYRH3XQtcC3DiiSeWu42s22aU+7GHI++evrG0C/E9Z5F0dST8jsS2AdnM6gPPATe6+9bC97v7eHfv6O4dMzMzkx9QROQIEsuagZnVICgET7n783FkkPTzSLt+cUcQSVlJLwZmZsDfgBXu/udkty/p6+NjtOlJpLzi6CbqBlwFfNfMcsM/HToqhy17/Ydkr/8w7hgiKSnpawbu/m/Akt2upL+BH8wGdMYzkfLQEcgiIqJiICIiKgYiIoKKgYiIoCGsJY2MzR4QdwSRlKViIGljXYPj4o4gkrLUTSRpo/Pny+j8+bK4Y4ikJK0ZSNq4ZNVrACxs0ibmJCKpR2sGIiKiYiAiIioGIiKCioGIiKANyJJG7jvjirgjiKQsFQNJGxvrHhN3BJGUpW4iSRvd1+bSfW1u3DFEUpLWDCqJTkqffH1Xvw7A3ObZSWszzvc5mSdLl/SnNQMREVExEBERFQMREUHFQERE0AZkSSOjzxwcdwSRlKViIGljawob3Z8AAAoOSURBVK16cUcQSVnqJpK00eeTRfT5ZFHcMURSkoqBpI1z1yzi3DUqBiLloWIgIiIqBiIiomIgIiKoGIiICNq1VNLIHWcNjTuCSMpSMZC0sTujZtwRRFKWuokkbfT9eD59P54fdwyRlKRiIGmj+7q36b7u7bhjiKQkFQMREYmnGJjZ98zsAzNbZWa3xZFBREQOSHoxMLPqwH8D3wdOAwaa2WnJziEiIgfEsWZwJrDK3T929z3AM0C/GHKIiEjI3D25DZoNAL7n7kPD61cBnd39+kLTXQtcG149BfggwuwbAxsrMG6yKHdyKXdyKXdyJeY+yd0zozwojuMMrIjbDqlI7j4eGF+mGZstdveO5Q0WF+VOLuVOLuVOrvLmjqObaC1wQsL15sBnMeQQEZFQHMVgEdDKzFqYWU3gR8D0GHKIiEgo6d1E7r7PzK4HXgaqA4+6+7IKmn2ZupWqEOVOLuVOLuVOrnLlTvoGZBERqXp0BLKIiKgYiIhIihYDM7vJzJaZ2XtmNtnMahcz3QAzczOrEruHRcltZj80s+XhdE/HkbOw0nKb2Ylm9n9mttTM3jGzC+LKmsjMbggzLzOzG4u438xsbDgsyjtmlhNHzsIi5B4U5n3HzF43s9PjyFkoU4mZE6brZGb7w+ONYhclt5n1NLPccJrXkp2xKBE+I0eb2Ytm9nY4zY9Lnam7p9Qf0AxYDdQJrz8LDCliugbAXOANoGMq5AZaAUuBY8Prx6VI7vHAz8LLpwF5VSB3W+A9oC7BjhKzgVaFprkA+AfBsS9dgIUpkrtrwmfk+3HnjpI5nK468CowExiQIq/1McBy4MTwelX4TkbJ/Rvg3vByJrAZqFnSfFNyzYDgBahjZhkEL0hRxyn8Afh/wK5kBitFabmHAf/t7l8CuPv6JOcrTmm5HTgqvHx0EffH4VTgDXff4e77gNeA/oWm6Qc84YE3gGPMrEmygxZSam53fz3/M0KwsNM8yRkLi/JaA/wCeA6oKp/rKLmvAJ539zVQZb6TUXI70MDMDKhPUAz2lTTTlCsG7r4OuB9YA3wOfOXuryROY2YdgBPc/aUYIhYpSm7gZOBkM5tvZm+Y2feSnbOwiLnvBK40s7UES32/SGrIor0HdDezRmZWl2At4IRC0zQDPk24vja8LU5Rcif6CcHaTZxKzWxmzQh+sMbFkK84UV7rk4FjzWyOmS0xs6uTnvJQUXI/RFA0PgPeBW5w929KmmnKFQMzO5Zgia4F0BSoZ2ZXJtxfDXgA+FU8CYtWWu5QBkFXUU9gIDDBzI5JZs7CIuYeCEx09+YEH8xJ4fsQG3dfAdwLzAL+CbzNoUtGkYZGSaaIuQEws14ExeDWpAUsQsTMY4Bb3X1/kuMVK2LuDOAMoC9wPvA7Mzs5mTkLi5j7fCCX4DubDTxkZkdRgpQrBkAfYLW7b3D3vcDzBH2o+RoQ9KnNMbM8gr7g6VVgI3JpuSFYMn3B3fe6+2qCwflaJTlnYVFy/4RgWwLuvgCoTTBYVqzc/W/unuPu3QlWk1cWmqRKDo0SITdm1h6YAPRz903JzlhYhMwdgWfC7+QA4GEz+0GSYx4i4mfkn+7+tbtvJNgOGfsG+wi5f0zQveXuvopgu1/rkuaZisVgDdDFzOqG/WG9gRX5d7r7V+7e2N2z3D2LoE/1YndfHE/cAiXmDk0DegGYWWOCVdSPk5ryUFFyrwlvx8xOJSgGG5Kasghmdlz4/0TgEmByoUmmA1eHexV1IegC+zzJMQ9RWu7w9ueBq9z9w+QnPFRpmd29RcJ3cipwnbtPS3rQQiJ8Rl4AzjGzjLBLpjOHfv6TLkLuxO/k8QQjP5f4WxLHqKWHxd0XmtlU4C2CVaOlwHgzGwUsdvcqOc5RxNwvA+eZ2XJgP3Bz3Et9EXP/Cvirmd1E0M0yxMPdGGL2nJk1AvYCP3f3L81sOIC7jyPYvnEBsArYQbA0VRWUlvsOoBHB0jXAPo9/dM3SMldVJeZ29xVm9k/gHeAbYIK7vxdj3nylvd5/ACaa2bsE3aG3hms2xdJwFCIikpLdRCIiUsFUDERERMVARERUDEREBBUDERFBxUCqGDP7lpk9Y2YfWTB668zyHvFpZiPMbIWZPWVmtcxsdjj65OVmNsHMTivhsReb2W3lbPcYM7uuPI8tZn7bK2peIsXRrqVSZYQHtb0OPJ6/b7qZZQMN3H1eOeb3PvB9d18dHlR2r7v3qNDQRbebBbzk7m0raH7b3b1+RcxLpDhaM5CqpBewN/EgJXfPdfd54VHC91kwhvu7ZnZ5/jRmdrOZLbJgfP+7wtvGAS0JhiK5FXgSyA7XDL4dDjzWMZz2e2b2lgVjv/8rvG2ImT0UXs40s+fCNhaZWbfw9jvN7NFwXh+b2Ygw0j3At8O27kt8gmZ2b+JaQziPX5lZfTP7V5jjXTPrV/jFsWBc/ZcSrj9kZkPCy2eY2WsWDKb2soWjr4ZrR8vD1+aZ8r4xkv5S7ghkSWttgSXF3HcJwYBbpxOMe7TIzOYC7QjGbzqT4EjL6WbW3d2HWzDqay9332hmC4GR7n4hQHjkLmaWCfwV6B6uQTQsou2/AA+4+7/Dw/9fJhgREoLxXnoRjIn1gZn9D3Ab0Nbds4uY1zMEg7Y9HF7/IfA9gqHW+7v7VguGInnDzKZHOZLbzGoADxKMU7QhLJSjgWvCLC3cfbfFPOihVG0qBpIqzgYmh6Ne/seCM051AroD5xEMkwHB2O2tCAYUi6ILMDccGBB331zENH2A0/ILCHCUmTUIL89w993AbjNbDxxfUmPuvtTMjjOzpgQnHfnS3deEP+h/NLPuBMMeNAvn9UWE53AKQSGdFWasTjDcOATDKDxlZtMIxr4SKZKKgVQlywhGtCxKUcNN59/+J3d/pJxtGqUPW10NOMvddx70wOCHd3fCTfuJ9p2aSvA8v0WwpgAwiKA4nOHuey0Y3bPwaVH3cXDXbv79Bixz97OKaKsvQcG8mGD45TbhCVFEDqJtBlKVvArUMrNh+TdYcM7cHgRL+pebWfWwa6c78CZBl801ZlY/nL6ZhSM6RrQA6GFmLcLHF9VN9ApwfUKmorp/Em0j6DYqzjPAjwgKwtTwtqOB9WEh6AWcVMTjPiFYQ6llZkcTjkpJMNR5ppmdFearYWZtLDinxAnu/n/ALQSncNSGaCmS1gykynB3N7P+wJhwt85dQB5wI0ExOIvgRB4O3OLuXwBfWDBs9oJwSX07cCURT60Y9rFfCzwf/niuB84tNNkI4L/N7B2C78xcYHgJ89xkwdnq3gP+4e43F7p/WdjNtC5hyOyngBfNbDHBSUneL2K+n5rZswRdPysJu8bcfY8FJ5gfGxaJDILtEh8CT4a3GcF2jy1RXhc58mjXUhERUTeRiIioGIiICCoGIiKCioGIiKBiICIiqBiIiAgqBiIiAvx/FqAnbEvtjgoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(coefficients, label='coefficients')\n",
    "\n",
    "plt.title(\"Histogram of coefficient values from CV\")\n",
    "plt.xlabel(\"Coefficient values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.axvline(LR1.coef_[0], c='C3', ls='--', label='mean of 25-fold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph above, we see the frequency of the different coefficient values. The coefficient values range from 8.4 to 9.7. The coefficient value from 1.1 is 8.96. We see that this value lies at the highest frequency coefficient value from the graph. I guess this should make sense because the coefficient value form 1.1 is taken from all the observations/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the two regression lines from 1.1 (or 1.2 if you prefer to do so). Show the linear regression line in red, and the linear+quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines.\n",
    "\n",
    "Note: You can get 2 points even without the confidence bands (if the other lines are correct, the axes are labeled, the lines are labeled, the points are plotted correctly, and so forth). If you do everything perfectly including the confidence bands, you get 2 points. If you do something else wrong and would normally receive less than two points, the confidence intervals can replace lost points up to 0.5.\n",
    "\n",
    "Useful reference: [1](https://www.medcalc.org/manual/scatter_diagram_regression_line.php), [2](https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals)\n",
    "\n",
    "You can directly use some packages to calculate the bands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c26f17110>"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rm ,pred1, c='C3', label='linear')\n",
    "plt.scatter(rm,pred2, c='C0', label='quadratic')\n",
    "plt.scatter(rm, y, c = 'C2',label='observed')\n",
    "plt.title(\"Predictions on median housing prices from number of rooms per house\")\n",
    "plt.xlabel(\"Average number of rooms per dwelling\")\n",
    "plt.ylabel(\"Median value of homes in $1000's\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (average rooms per house)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "* How do your coefficients compare to the ones estimated through standard libraries? Does this depend on *R*?\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "ini_alpha: float\n",
    "    initial alpha\n",
    "\n",
    "ini_beta: float\n",
    "    initial beta\n",
    "\n",
    "epsilon: float\n",
    "    convergence threshold\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "    \n",
    "total_time: float\n",
    "    running time\n",
    "\n",
    "iterations: int\n",
    "    number of iterations\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "def bivariate_ols(xvalues, yvalues, ini_alpha, ini_beta, R, epsilon, MaxIterations):\n",
    "    # initialize the parameters\n",
    "    start_time = time.time()\n",
    "    \n",
    "    alpha = ini_alpha\n",
    "    beta = ini_beta\n",
    "\n",
    "    for i in range(MaxIterations):\n",
    "        \n",
    "        # update coefficients\n",
    "        alpha_new = alpha - ((R/len(yvalues)) * np.sum(alpha + beta * xvalues - yvalues))\n",
    "        beta_new = beta - ((R/len(yvalues)) * np.sum((alpha + beta * xvalues - yvalues) * xvalues))\n",
    "    \n",
    "        # check for stopping criteria\n",
    "        if (np.abs(alpha_new-alpha)<epsilon) & (np.abs(beta_new-beta)<epsilon):\n",
    "            alpha = alpha_new\n",
    "            beta = beta_new\n",
    "            break\n",
    "        else:\n",
    "            alpha = alpha_new\n",
    "            beta = beta_new\n",
    "\n",
    "    iterations = i+1\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return alpha, beta, total_time, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.0001\n",
      "alpha value = -10.160513232912816\n",
      "beta value  = 5.110124769602543\n",
      "time taken  = 0.0359189510345459s\n",
      "iterations  = 837\n",
      "R = 0.001\n",
      "alpha value = -12.885209905324688\n",
      "beta value  = 5.501123768457125\n",
      "time taken  = 0.30045199394226074s\n",
      "iterations  = 10000\n",
      "R = 0.01\n",
      "alpha value = -27.45887117765605\n",
      "beta value  = 7.722596446176507\n",
      "time taken  = 0.2824690341949463s\n",
      "iterations  = 10000\n"
     ]
    }
   ],
   "source": [
    "x = data['RM'].values\n",
    "y = data['MEDV'].values\n",
    "\n",
    "# initial alpha, beta\n",
    "ini_alpha = -10\n",
    "ini_beta = 6\n",
    "\n",
    "# R = 0.0001\n",
    "alpha3, beta3, total_time3 , iterations3 = bivariate_ols(x, y, ini_alpha, ini_beta, R=0.0001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.0001')\n",
    "print('alpha value = {}'.format(alpha3))\n",
    "print('beta value  = {}'.format(beta3))\n",
    "print('time taken  = {}s'.format(total_time3))\n",
    "print('iterations  = {}'.format(iterations3))\n",
    "\n",
    "# R = 0.001\n",
    "alpha2, beta2, total_time2 , iterations2 = bivariate_ols(x, y, ini_alpha, ini_beta, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.001')\n",
    "print('alpha value = {}'.format(alpha2))\n",
    "print('beta value  = {}'.format(beta2))\n",
    "print('time taken  = {}s'.format(total_time2))\n",
    "print('iterations  = {}'.format(iterations2))\n",
    "\n",
    "# R = 0.01\n",
    "alpha1, beta1, total_time1 , iterations1 = bivariate_ols(x, y, ini_alpha, ini_beta, R=0.01, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.01')\n",
    "print('alpha value = {}'.format(alpha1))\n",
    "print('beta value  = {}'.format(beta1))\n",
    "print('time taken  = {}s'.format(total_time1))\n",
    "print('iterations  = {}'.format(iterations1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a learning rate of 0.01, and 0.001, we find that we go through all 10,000 iterations. This is probably because the learning rates are too large, and so it is not able to converge. When we use a learning rate of 0.0001, we find that we only go throiugh 837 iterations. From 1.2, the linear model had a slope value of 8.96. From our model with the learning rate of 0.0001, we get a beta value of 5.11. What I also noticed is that when I change the initial alpha and beta values, we get different approximations from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using CRIM and RM as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of multivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "ini_alpha: float\n",
    "    initial alpha\n",
    "\n",
    "ini_beta: float\n",
    "    initial beta\n",
    "\n",
    "epsilon: float\n",
    "    convergence threshold\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "    \n",
    "total_time: float\n",
    "    running time\n",
    "\n",
    "iterations: int\n",
    "    number of iterations\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "def multivariate_ols(xvalues, yvalues, ini_alpha, ini_beta, R, epsilon, MaxIterations):\n",
    "    # initialize the parameters\n",
    "    start_time = time.time()\n",
    "    \n",
    "    alpha = ini_alpha\n",
    "    beta = ini_beta\n",
    "\n",
    "    for i in range(MaxIterations):\n",
    "        \n",
    "        # update coefficients\n",
    "        alpha_new = alpha - ((R/len(yvalues)) * np.sum(alpha + np.sum(beta * xvalues,axis=1) - yvalues))\n",
    "        beta_new = beta - ((R/len(yvalues)) * np.sum((alpha + np.sum(beta * xvalues,axis=1) - yvalues) * (xvalues.T),axis=1))\n",
    "    \n",
    "        # check for stopping criteria\n",
    "        if (np.abs(alpha_new-alpha)<epsilon) & (np.abs(beta_new-beta)<epsilon).all():\n",
    "            alpha = alpha_new\n",
    "            beta = beta_new\n",
    "            break\n",
    "        else:\n",
    "            alpha = alpha_new\n",
    "            beta = beta_new\n",
    "    \n",
    "    iterations = i+1\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return alpha, beta, total_time, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.0001\n",
      "alpha value = 10.565\n",
      "beta value  = 6.570,0.880\n",
      "time taken  = 0.5759470462799072s\n",
      "iterations  = 10000\n",
      "R = 0.001\n",
      "alpha value = 22.433\n",
      "beta value  = 5.862,-2.192\n",
      "time taken  = 0.35030031204223633s\n",
      "iterations  = 5783\n",
      "R = 0.01\n",
      "alpha value = 22.523\n",
      "beta value  = 5.825,-2.235\n",
      "time taken  = 0.049242258071899414s\n",
      "iterations  = 806\n"
     ]
    }
   ],
   "source": [
    "x = standardize(data[['RM','CRIM']].values)\n",
    "y = data['MEDV'].values\n",
    "\n",
    "# initial alpha, beta\n",
    "ini_alpha = -10\n",
    "ini_beta = [6,6]\n",
    "\n",
    "# R = 0.0001\n",
    "alpha3, beta3, total_time3 , iterations3 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.0001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.0001')\n",
    "print('alpha value = {:.3f}'.format(alpha3))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta3[0],beta3[1]))\n",
    "print('time taken  = {}s'.format(total_time3))\n",
    "print('iterations  = {}'.format(iterations3))\n",
    "\n",
    "# R = 0.001\n",
    "alpha2, beta2, total_time2 , iterations2 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.001')\n",
    "print('alpha value = {:.3f}'.format(alpha2))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta2[0],beta2[1]))\n",
    "print('time taken  = {}s'.format(total_time2))\n",
    "print('iterations  = {}'.format(iterations2))\n",
    "\n",
    "# R = 0.01\n",
    "alpha1, beta1, total_time1 , iterations1 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.01, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.01')\n",
    "print('alpha value = {:.3f}'.format(alpha1))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta1[0],beta1[1]))\n",
    "print('time taken  = {}s'.format(total_time1))\n",
    "print('iterations  = {}'.format(iterations1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the our values above, when we have a learning rate of 0.01, our model only requires 806 iterations. When we have a learning rate of 0.001, we go through 5783 iterations. Finally, when we have a learning rate of 0.0001, we go through all 10,000 iterations. This is the opposite scenario to what we found in 2.1. The alpha and beta values for R=0.01 and R=0.001 are relatively similar. This could be because the learning rate is so small, we need to go through more iterations for the model to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.0001\n",
      "alpha value = -10.162\n",
      "beta value  = 5.235,-0.314\n",
      "time taken  = 0.08112001419067383s\n",
      "iterations  = 1208\n",
      "R = 0.001\n",
      "alpha value = -12.131\n",
      "beta value  = 5.561,-0.320\n",
      "time taken  = 0.6050879955291748s\n",
      "iterations  = 10000\n",
      "R = 0.01\n",
      "alpha value = -23.112\n",
      "beta value  = 7.215,-0.284\n",
      "time taken  = 0.6290569305419922s\n",
      "iterations  = 10000\n"
     ]
    }
   ],
   "source": [
    "x = data[['RM','CRIM']].values\n",
    "y = data['MEDV'].values\n",
    "\n",
    "# initial alpha, beta\n",
    "ini_alpha = -10\n",
    "ini_beta = [6,6]\n",
    "\n",
    "# R = 0.0001\n",
    "alpha3, beta3, total_time3 , iterations3 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.0001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.0001')\n",
    "print('alpha value = {:.3f}'.format(alpha3))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta3[0],beta3[1]))\n",
    "print('time taken  = {}s'.format(total_time3))\n",
    "print('iterations  = {}'.format(iterations3))\n",
    "\n",
    "# R = 0.001\n",
    "alpha2, beta2, total_time2 , iterations2 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.001')\n",
    "print('alpha value = {:.3f}'.format(alpha2))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta2[0],beta2[1]))\n",
    "print('time taken  = {}s'.format(total_time2))\n",
    "print('iterations  = {}'.format(iterations2))\n",
    "\n",
    "# R = 0.01\n",
    "alpha1, beta1, total_time1 , iterations1 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.01, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.01')\n",
    "print('alpha value = {:.3f}'.format(alpha1))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta1[0],beta1[1]))\n",
    "print('time taken  = {}s'.format(total_time1))\n",
    "print('iterations  = {}'.format(iterations1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do not standardize the variables, the learning rate of 0.01 and 0.001 go through all 10,000 iterations, while the learning rate of 0.0001 goes through 1208 iterations. This is the opposite from the case in 2.3. Because the learning rate of 0.001 and 0.01 goes through all 10,000 iterations, they have a longer running time compared t0 in 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Implement  Mini-Batch Gradient Descent (MB-GD)**\n",
    "MB-GD is a Gradient Descent variant that in large data sets can converge faster and is computationally less intensive. Implement MB-GD for question 2.3. Tune the learning rate, number of iterations and \"mini-batch\" size\n",
    "so that you compute the estimates within a 1e-2 tolerance. Do not use a batch-size greater than 32.\n",
    "In summary you go over the entire data set for n epochs. At the beginning of each epoch you shuffle your data once and then you pick k batches (approximately k=#of data points/batch_size). For each batch you compute the gradient, update the parameters and move to the next batch.\n",
    "MB-GD is similar to Stochastic Gradient Descent but instead of using one sample to compute the gradient we use a batch of samples at each iteration. You can find details about MB-GD here:\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MB_GD_ols(xvalues, yvalues, ini_alpha, ini_beta, num_batch, R, epsilon, MaxIterations):\n",
    "    # initialize the parameters\n",
    "    start_time = time.time()\n",
    "    \n",
    "    alpha = ini_alpha\n",
    "    beta = ini_beta\n",
    "    \n",
    "    for i in range(MaxIterations):\n",
    "        # Shuffle and split into batches\n",
    "        kf = KFold(n_splits=num_batch,shuffle=True)\n",
    "        batch_index = []\n",
    "        for a, batch in kf.split(x):\n",
    "            batch_index.append(batch)\n",
    "        \n",
    "        for j in range(num_batch):\n",
    "        \n",
    "            # update coefficients\n",
    "            alpha_new = alpha - ((R/len(yvalues[batch_index[j]])) * np.sum(alpha + np.sum(beta * xvalues[batch_index[j]],axis=1) - yvalues[batch_index[j]]))\n",
    "            beta_new = beta - ((R/len(yvalues[batch_index[j]])) * np.sum((alpha + np.sum(beta * xvalues[batch_index[j]],axis=1) - yvalues[batch_index[j]]) * (xvalues[batch_index[j]].T),axis=1))\n",
    "    \n",
    "        # check for stopping criteria\n",
    "            if (np.abs(alpha_new-alpha)<epsilon) & (np.abs(beta_new-beta)<epsilon).all():\n",
    "                alpha = alpha_new\n",
    "                beta = beta_new\n",
    "                break\n",
    "            else:\n",
    "                alpha = alpha_new\n",
    "                beta = beta_new\n",
    "    \n",
    "        iterations = i+1\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return alpha, beta, total_time, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.0001, 25 batches, 10000 iterations\n",
      "alpha value = 10.561\n",
      "beta value  = 6.602,0.880\n",
      "time taken  = 5.508195638656616s\n",
      "iterations  = 10000\n",
      "R = 0.0001, 20 batches, 10000 iterations\n",
      "alpha value = 10.551\n",
      "beta value  = 6.546,0.887\n",
      "time taken  = 5.285573244094849s\n",
      "iterations  = 10000\n",
      "R = 0.001, 25 batches, 10000 iterations\n",
      "alpha value = 22.579\n",
      "beta value  = 5.817,-2.215\n",
      "time taken  = 5.2973408699035645s\n",
      "iterations  = 10000\n",
      "R = 0.001, 20 batches, 10000 iterations\n",
      "alpha value = 22.543\n",
      "beta value  = 5.841,-2.241\n",
      "time taken  = 4.992336273193359s\n",
      "iterations  = 10000\n",
      "R = 0.01, 25 batches, 10000 iterations\n",
      "alpha value = 22.534\n",
      "beta value  = 5.911,-2.244\n",
      "time taken  = 5.046231031417847s\n",
      "iterations  = 10000\n",
      "R = 0.01, 20 batches, 10000 iterations\n",
      "alpha value = 22.476\n",
      "beta value  = 5.849,-2.228\n",
      "time taken  = 5.669514894485474s\n",
      "iterations  = 10000\n"
     ]
    }
   ],
   "source": [
    "x = standardize(data[['RM','CRIM']].values)\n",
    "y = data['MEDV'].values\n",
    "\n",
    "# initial alpha, beta\n",
    "ini_alpha = -10\n",
    "ini_beta = [6,6]\n",
    "\n",
    "# R = 0.0001, 25 batches, 10000 iterations\n",
    "alpha1, beta1, total_time1 , iterations1 = MB_GD_ols(x, y, ini_alpha, ini_beta,num_batch = 25, R=0.0001, epsilon=0.01, MaxIterations=10000)\n",
    "print('R = 0.0001, 25 batches, 10000 iterations')\n",
    "print('alpha value = {:.3f}'.format(alpha1))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta1[0],beta1[1]))\n",
    "print('time taken  = {}s'.format(total_time1))\n",
    "print('iterations  = {}'.format(iterations1))\n",
    "\n",
    "# R = 0.0001, 20 batches, 10000 iterations\n",
    "alpha2, beta2, total_time2 , iterations2 = MB_GD_ols(x, y, ini_alpha, ini_beta,num_batch = 25, R=0.0001, epsilon=0.01, MaxIterations=10000)\n",
    "print('R = 0.0001, 20 batches, 10000 iterations')\n",
    "print('alpha value = {:.3f}'.format(alpha2))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta2[0],beta2[1]))\n",
    "print('time taken  = {}s'.format(total_time2))\n",
    "print('iterations  = {}'.format(iterations2))\n",
    "\n",
    "# R = 0.001, 25 batches, 10000 iterations\n",
    "alpha3, beta3, total_time3 , iterations3 = MB_GD_ols(x, y, ini_alpha, ini_beta,num_batch = 25, R=0.001, epsilon=0.01, MaxIterations=10000)\n",
    "print('R = 0.001, 25 batches, 10000 iterations')\n",
    "print('alpha value = {:.3f}'.format(alpha3))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta3[0],beta3[1]))\n",
    "print('time taken  = {}s'.format(total_time3))\n",
    "print('iterations  = {}'.format(iterations3))\n",
    "\n",
    "# R = 0.001, 20 batches, 10000 iterations\n",
    "alpha4, beta4, total_time4 , iterations4 = MB_GD_ols(x, y, ini_alpha, ini_beta,num_batch = 25, R=0.001, epsilon=0.01, MaxIterations=10000)\n",
    "print('R = 0.001, 20 batches, 10000 iterations')\n",
    "print('alpha value = {:.3f}'.format(alpha4))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta4[0],beta4[1]))\n",
    "print('time taken  = {}s'.format(total_time4))\n",
    "print('iterations  = {}'.format(iterations4))\n",
    "\n",
    "# R = 0.01, 25 batches, 10000 iterations\n",
    "alpha5, beta5, total_time5 , iterations5 = MB_GD_ols(x, y, ini_alpha, ini_beta,num_batch = 25, R=0.001, epsilon=0.01, MaxIterations=10000)\n",
    "print('R = 0.01, 25 batches, 10000 iterations')\n",
    "print('alpha value = {:.3f}'.format(alpha5))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta5[0],beta5[1]))\n",
    "print('time taken  = {}s'.format(total_time5))\n",
    "print('iterations  = {}'.format(iterations5))\n",
    "\n",
    "# R = 0.01, 20 batches, 10000 iterations\n",
    "alpha6, beta6, total_time6 , iterations6 = MB_GD_ols(x, y, ini_alpha, ini_beta,num_batch = 25, R=0.001, epsilon=0.01, MaxIterations=10000)\n",
    "print('R = 0.01, 20 batches, 10000 iterations')\n",
    "print('alpha value = {:.3f}'.format(alpha6))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta6[0],beta6[1]))\n",
    "print('time taken  = {}s'.format(total_time6))\n",
    "print('iterations  = {}'.format(iterations6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tuning through the learning rates and batch sizes, we find that our running time did not speed up for convergence. For example,  from 2.3, our running time for a learning rate. of 0,01 was 0.04 seconds. In the case of the MB-GD, with the same learning rate of 0.01, our running time increased and went through the max number of iterations. The MB-GD function is made for each epoch to shuffle the data once and then pick k batches. For each batch, the gradient is computed, parameters updated, and moved to the next batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Make sure to first standardize your features before proceeding.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again. Let's fix that in one of two ways. If you're feeling confident, use k-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three, corresponding to the intercept and the two coefficients for CRIM and RM). Or if you want to do the quick and dirty version, randomly divide your data into a training set (66%) and testing set (34%) and use the training set to re-fit the regression from 2.3 above. \n",
    "\n",
    "*NOTE: * If using k-fold cross-validation, you will end up with a different set of parameters for each fold. In this case, use the parameters from the fold that has the highest test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.001, 66% training set\n",
      "alpha value = 22.360\n",
      "beta value  = 5.774,-2.060\n",
      "time taken  = 0.38711094856262207s\n",
      "iterations  = 5781\n",
      "R = 0.001, Q2.3, all training set\n",
      "alpha value = 22.433\n",
      "beta value  = 5.862,-2.192\n",
      "time taken  = 0.3760650157928467s\n",
      "iterations  = 5783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.34)\n",
    "\n",
    "x_train = standardize(train[['RM','CRIM']].values)\n",
    "y_train = train['MEDV'].values\n",
    "\n",
    "ini_alpha = -10\n",
    "ini_beta = [6,6]\n",
    "\n",
    "# R = 0.001 using 66% training set\n",
    "alpha1, beta1, total_time1 , iterations1 = multivariate_ols(x_train, y_train, ini_alpha, ini_beta, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.001, 66% training set')\n",
    "print('alpha value = {:.3f}'.format(alpha1))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta1[0],beta1[1]))\n",
    "print('time taken  = {}s'.format(total_time1))\n",
    "print('iterations  = {}'.format(iterations1))\n",
    "\n",
    "# R = 0.001, part from 2.3. Using all training set\n",
    "alpha2, beta2, total_time2 , iterations2 = multivariate_ols(x, y, ini_alpha, ini_beta, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.001, Q2.3, all training set')\n",
    "print('alpha value = {:.3f}'.format(alpha2))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta2[0],beta2[1]))\n",
    "print('time taken  = {}s'.format(total_time2))\n",
    "print('iterations  = {}'.format(iterations2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient values are around the same when using the 66% of the data, and using all of the data. When using 66% of the data for training, the model used less iterations. There is also no significant difference in running time. This is a good sign to see that we have similar coefficient values when training on less observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Compute the RMSE on your test cases, i.e. take the model parameters that you found above, predict the values for just the test instances, and compare the actual to the predicted values. If you did this the k-fold way above, this will be the average RMSE across the k test sets. If you did this the quick and dirty way above, this will just be the RMSE on your single test set.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = standardize(test[['RM','CRIM']].values)\n",
    "y_test = test[['MEDV']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y_{predict} = \\alpha + x_{1}\\beta_{1} + x_{2}\\beta_{2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value for prediction = 147.32412596663806\n"
     ]
    }
   ],
   "source": [
    "y_predict = (alpha1) + (x_test[:,0]*beta1[0]) + (x_test[:,1]*beta1[1])\n",
    "RMSE = compute_rmse(y_predict, y_test)\n",
    "\n",
    "print('RMSE value for prediction = {}'.format(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test RMSE value we get is 6.8023. This value is much lower than the RMSE value 10.21 that we got from the nearest neighbor algorithm in the last problem. As mentioned from 3.1, the model trained that used 66% of the training data gave coefficient values that were very similar to the values from training on 100% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit 1: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $40,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: Create new interaction variables between each possible pair of the F_s features. If you originally had *K* features, you should now have K+(K*(K+1))/2 features. Standardize all of your features.\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>15.534711</td>\n",
       "      <td>397.462329</td>\n",
       "      <td>5.715647</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>17.914131</td>\n",
       "      <td>397.012611</td>\n",
       "      <td>9.338417</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>17.919989</td>\n",
       "      <td>396.628236</td>\n",
       "      <td>4.142473</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.979527</td>\n",
       "      <td>398.564784</td>\n",
       "      <td>3.239272</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>18.708888</td>\n",
       "      <td>399.487766</td>\n",
       "      <td>6.115159</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX    PTRATIO           B     LSTAT  MEDV  \n",
       "0  307.0  15.534711  397.462329  5.715647  24.0  \n",
       "1  255.0  17.914131  397.012611  9.338417  21.6  \n",
       "2  243.0  17.919989  396.628236  4.142473  34.7  \n",
       "3  226.0  18.979527  398.564784  3.239272  33.4  \n",
       "4  234.0  18.708888  399.487766  6.115159  36.2  "
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>15.534711</td>\n",
       "      <td>397.462329</td>\n",
       "      <td>5.715647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>17.914131</td>\n",
       "      <td>397.012611</td>\n",
       "      <td>9.338417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>17.919989</td>\n",
       "      <td>396.628236</td>\n",
       "      <td>4.142473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.979527</td>\n",
       "      <td>398.564784</td>\n",
       "      <td>3.239272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>18.708888</td>\n",
       "      <td>399.487766</td>\n",
       "      <td>6.115159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX    PTRATIO           B     LSTAT  \n",
       "0  307.0  15.534711  397.462329  5.715647  \n",
       "1  255.0  17.914131  397.012611  9.338417  \n",
       "2  243.0  17.919989  396.628236  4.142473  \n",
       "3  226.0  18.979527  398.564784  3.239272  \n",
       "4  234.0  18.708888  399.487766  6.115159  "
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = data.drop(['MEDV'],axis=1)\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "num_features = len(x_data.columns)\n",
    "for i in range(num_features):\n",
    "    for j in range(i, num_features):\n",
    "        col = x_data.columns[i] +'*'+ x_data.columns[j]\n",
    "        x_data[col] = x_data.iloc[:,i] * x_data.iloc[:,i]\n",
    "        \n",
    "print(len(x_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>TAX*TAX</th>\n",
       "      <th>TAX*PTRATIO</th>\n",
       "      <th>TAX*B</th>\n",
       "      <th>TAX*LSTAT</th>\n",
       "      <th>PTRATIO*PTRATIO</th>\n",
       "      <th>PTRATIO*B</th>\n",
       "      <th>PTRATIO*LSTAT</th>\n",
       "      <th>B*B</th>\n",
       "      <th>B*LSTAT</th>\n",
       "      <th>LSTAT*LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>241.327253</td>\n",
       "      <td>241.327253</td>\n",
       "      <td>241.327253</td>\n",
       "      <td>157976.303016</td>\n",
       "      <td>157976.303016</td>\n",
       "      <td>32.668615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65025.0</td>\n",
       "      <td>65025.0</td>\n",
       "      <td>65025.0</td>\n",
       "      <td>65025.0</td>\n",
       "      <td>320.916074</td>\n",
       "      <td>320.916074</td>\n",
       "      <td>320.916074</td>\n",
       "      <td>157619.013024</td>\n",
       "      <td>157619.013024</td>\n",
       "      <td>87.206031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59049.0</td>\n",
       "      <td>59049.0</td>\n",
       "      <td>59049.0</td>\n",
       "      <td>59049.0</td>\n",
       "      <td>321.126013</td>\n",
       "      <td>321.126013</td>\n",
       "      <td>321.126013</td>\n",
       "      <td>157313.957558</td>\n",
       "      <td>157313.957558</td>\n",
       "      <td>17.160083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51076.0</td>\n",
       "      <td>51076.0</td>\n",
       "      <td>51076.0</td>\n",
       "      <td>51076.0</td>\n",
       "      <td>360.222430</td>\n",
       "      <td>360.222430</td>\n",
       "      <td>360.222430</td>\n",
       "      <td>158853.886758</td>\n",
       "      <td>158853.886758</td>\n",
       "      <td>10.492886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54756.0</td>\n",
       "      <td>54756.0</td>\n",
       "      <td>54756.0</td>\n",
       "      <td>54756.0</td>\n",
       "      <td>350.022485</td>\n",
       "      <td>350.022485</td>\n",
       "      <td>350.022485</td>\n",
       "      <td>159590.475176</td>\n",
       "      <td>159590.475176</td>\n",
       "      <td>37.395171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX  ...  TAX*TAX  TAX*PTRATIO    TAX*B  TAX*LSTAT  PTRATIO*PTRATIO  \\\n",
       "0  307.0  ...  94249.0      94249.0  94249.0    94249.0       241.327253   \n",
       "1  255.0  ...  65025.0      65025.0  65025.0    65025.0       320.916074   \n",
       "2  243.0  ...  59049.0      59049.0  59049.0    59049.0       321.126013   \n",
       "3  226.0  ...  51076.0      51076.0  51076.0    51076.0       360.222430   \n",
       "4  234.0  ...  54756.0      54756.0  54756.0    54756.0       350.022485   \n",
       "\n",
       "    PTRATIO*B  PTRATIO*LSTAT            B*B        B*LSTAT  LSTAT*LSTAT  \n",
       "0  241.327253     241.327253  157976.303016  157976.303016    32.668615  \n",
       "1  320.916074     320.916074  157619.013024  157619.013024    87.206031  \n",
       "2  321.126013     321.126013  157313.957558  157313.957558    17.160083  \n",
       "3  360.222430     360.222430  158853.886758  158853.886758    10.492886  \n",
       "4  350.022485     350.022485  159590.475176  159590.475176    37.395171  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data['MEDV'] = data['MEDV']\n",
    "\n",
    "train, test = train_test_split(x_data, test_size=0.34, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>TAX*PTRATIO</th>\n",
       "      <th>TAX*B</th>\n",
       "      <th>TAX*LSTAT</th>\n",
       "      <th>PTRATIO*PTRATIO</th>\n",
       "      <th>PTRATIO*B</th>\n",
       "      <th>PTRATIO*LSTAT</th>\n",
       "      <th>B*B</th>\n",
       "      <th>B*LSTAT</th>\n",
       "      <th>LSTAT*LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.687641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.406245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>7.428125</td>\n",
       "      <td>71.6</td>\n",
       "      <td>4.347051</td>\n",
       "      <td>8.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98596.0</td>\n",
       "      <td>98596.0</td>\n",
       "      <td>98596.0</td>\n",
       "      <td>307.253497</td>\n",
       "      <td>307.253497</td>\n",
       "      <td>307.253497</td>\n",
       "      <td>154917.835393</td>\n",
       "      <td>154917.835393</td>\n",
       "      <td>24.951474</td>\n",
       "      <td>31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>4.047467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.319182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620994</td>\n",
       "      <td>6.239441</td>\n",
       "      <td>90.7</td>\n",
       "      <td>3.101021</td>\n",
       "      <td>24.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>...</td>\n",
       "      <td>448900.0</td>\n",
       "      <td>448900.0</td>\n",
       "      <td>448900.0</td>\n",
       "      <td>418.077306</td>\n",
       "      <td>418.077306</td>\n",
       "      <td>418.077306</td>\n",
       "      <td>156340.739283</td>\n",
       "      <td>156340.739283</td>\n",
       "      <td>175.271989</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.269147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.957769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787734</td>\n",
       "      <td>6.498970</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.890210</td>\n",
       "      <td>3.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54289.0</td>\n",
       "      <td>54289.0</td>\n",
       "      <td>54289.0</td>\n",
       "      <td>329.166981</td>\n",
       "      <td>329.166981</td>\n",
       "      <td>329.166981</td>\n",
       "      <td>156551.588791</td>\n",
       "      <td>156551.588791</td>\n",
       "      <td>64.635221</td>\n",
       "      <td>24.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.333204</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.828966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.836908</td>\n",
       "      <td>6.521196</td>\n",
       "      <td>21.1</td>\n",
       "      <td>6.927974</td>\n",
       "      <td>4.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>...</td>\n",
       "      <td>63504.0</td>\n",
       "      <td>63504.0</td>\n",
       "      <td>63504.0</td>\n",
       "      <td>282.552984</td>\n",
       "      <td>282.552984</td>\n",
       "      <td>282.552984</td>\n",
       "      <td>158512.941262</td>\n",
       "      <td>158512.941262</td>\n",
       "      <td>33.772276</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.201968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.628632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700411</td>\n",
       "      <td>5.694222</td>\n",
       "      <td>89.8</td>\n",
       "      <td>3.250913</td>\n",
       "      <td>3.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37249.0</td>\n",
       "      <td>37249.0</td>\n",
       "      <td>37249.0</td>\n",
       "      <td>323.680511</td>\n",
       "      <td>323.680511</td>\n",
       "      <td>323.680511</td>\n",
       "      <td>154692.879011</td>\n",
       "      <td>154692.879011</td>\n",
       "      <td>204.639551</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN      INDUS  CHAS       NOX        RM   AGE       DIS  \\\n",
       "237  0.687641   0.0   6.406245   0.0  0.674074  7.428125  71.6  4.347051   \n",
       "471  4.047467   0.0  18.319182   0.0  0.620994  6.239441  90.7  3.101021   \n",
       "43   0.269147   0.0   6.957769   0.0  0.787734  6.498970   6.5  5.890210   \n",
       "52   0.333204  21.0   5.828966   0.0  0.836908  6.521196  21.1  6.927974   \n",
       "184  0.201968   0.0   2.628632   0.0  0.700411  5.694222  89.8  3.250913   \n",
       "\n",
       "      RAD    TAX  ...  TAX*PTRATIO     TAX*B  TAX*LSTAT  PTRATIO*PTRATIO  \\\n",
       "237   8.0  314.0  ...      98596.0   98596.0    98596.0       307.253497   \n",
       "471  24.0  670.0  ...     448900.0  448900.0   448900.0       418.077306   \n",
       "43    3.0  233.0  ...      54289.0   54289.0    54289.0       329.166981   \n",
       "52    4.0  252.0  ...      63504.0   63504.0    63504.0       282.552984   \n",
       "184   3.0  193.0  ...      37249.0   37249.0    37249.0       323.680511   \n",
       "\n",
       "      PTRATIO*B  PTRATIO*LSTAT            B*B        B*LSTAT  LSTAT*LSTAT  \\\n",
       "237  307.253497     307.253497  154917.835393  154917.835393    24.951474   \n",
       "471  418.077306     418.077306  156340.739283  156340.739283   175.271989   \n",
       "43   329.166981     329.166981  156551.588791  156551.588791    64.635221   \n",
       "52   282.552984     282.552984  158512.941262  158512.941262    33.772276   \n",
       "184  323.680511     323.680511  154692.879011  154692.879011   204.639551   \n",
       "\n",
       "     MEDV  \n",
       "237  31.5  \n",
       "471  19.6  \n",
       "43   24.7  \n",
       "52   25.0  \n",
       "184  26.4  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Overfitting (sort of)\n",
    "Now, using your version of multivariate regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those K+(K*(K+1))/2 features as you can (Don't forget to add quadratic terms. Form instance, RM^2.).  If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.001\n",
      "time taken to find coefficients  = 0.8666369915008545s\n",
      "iterations  = 9192\n"
     ]
    }
   ],
   "source": [
    "# We regress on the first 30 features\n",
    "\n",
    "x_train = standardize(train.iloc[:,0:30].values)\n",
    "y_train = train['MEDV'].values\n",
    "\n",
    "x_test = standardize(test.iloc[:,0:30].values)\n",
    "y_test = test['MEDV'].values\n",
    "\n",
    "# Initial beta values is an array of 2\n",
    "ini_alpha = -10\n",
    "ini_beta = np.zeros(30)+2\n",
    "\n",
    "train_alpha, train_beta, train_total_time, train_iterations = multivariate_ols(x_train, y_train, ini_alpha, ini_beta, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "print('R = 0.001')\n",
    "print('time taken to find coefficients  = {}s'.format(train_total_time))\n",
    "print('iterations  = {}'.format(train_iterations))\n",
    "\n",
    "# Initialize train_y_predict to alpha value, and then we add all the betas through the for loop\n",
    "train_y_predict = train_alpha\n",
    "\n",
    "for i in range(0,30):\n",
    "    train_y_predict += x_train[:,i]*train_beta[i]\n",
    "\n",
    "# Initialize test_y_predict to alpha value, and then we add all the betas through the for loop on the test data\n",
    "test_y_predict = train_alpha\n",
    "\n",
    "for i in range(0,30):\n",
    "    test_y_predict += x_test[:,i]*train_beta[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RMSE cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value on training set = 4.850498423654782\n",
      "RMSE value on testing set = 4.814002127320451\n"
     ]
    }
   ],
   "source": [
    "RMSE_train = compute_rmse(train_y_predict, y_train)\n",
    "RMSE_test = compute_rmse(test_y_predict, y_test)\n",
    "\n",
    "print('RMSE value on training set = {}'.format(RMSE_train))\n",
    "print('RMSE value on testing set = {}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE value for the training set is 4.526. The RMSE value for the testing set is 5.3066. These values are less than the RMSE value we got from 3.2 which was 6.802. Because the model is very overfitted, we should normally expect a much higher RMSE value on the testing set. However, the RMSE value on the testing set is not that much higher than the RMSE on the training set, and it is still lower than the value in 3.2. This value is also lower than the RMSE value of 10.21 from the nearest neighbors problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model from 4.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "Go brag to your friends about how you just implemented ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_multivariate_ols(xvalues, yvalues, ini_alpha, ini_beta, lamda, R, epsilon, MaxIterations):\n",
    "    # initialize the parameters\n",
    "    start_time = time.time()\n",
    "    \n",
    "    alpha = ini_alpha\n",
    "    beta = ini_beta\n",
    "\n",
    "    for i in range(MaxIterations):\n",
    "        \n",
    "        # update coefficients\n",
    "        alpha_new = alpha - ((R/len(yvalues)) * np.sum(alpha + np.sum(beta * xvalues,axis=1) - yvalues))\n",
    "        beta_new = (beta*(1-((lamda/len(yvalues))*R))) - ((R/len(yvalues)) * np.sum((alpha + np.sum(beta * xvalues,axis=1) - yvalues) * (xvalues.T),axis=1))\n",
    "    \n",
    "        # check for stopping criteria\n",
    "        if (np.abs(alpha_new-alpha)<epsilon) & (np.abs(beta_new-beta)<epsilon).all():\n",
    "            alpha = alpha_new\n",
    "            beta = beta_new\n",
    "            break\n",
    "        else:\n",
    "            alpha = alpha_new\n",
    "            beta = beta_new\n",
    "    \n",
    "    iterations = i+1\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return alpha, beta, total_time, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using same model from 4.2 that has the first 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.0001, lambda = 1\n",
      "alpha value = 10.838\n",
      "beta value  = -0.378,0.502\n",
      "time taken  = 1.0191171169281006s\n",
      "iterations  = 10000\n",
      "RMSE value on training set = 13.506393748618953\n",
      "RMSE value on testing set = 12.238873540476634\n",
      "\n",
      "R = 0.0001, lambda = 0.1\n",
      "alpha value = 10.838\n",
      "beta value  = -0.378,0.503\n",
      "time taken  = 1.0980238914489746s\n",
      "iterations  = 10000\n",
      "RMSE value on training set = 13.507167694316287\n",
      "RMSE value on testing set = 12.240092118998312\n",
      "\n",
      "R = 0.001, lambda = 0.01\n",
      "alpha value = 22.961\n",
      "beta value  = 0.127,0.004\n",
      "time taken  = 0.8798139095306396s\n",
      "iterations  = 9192\n",
      "RMSE value on training set = 4.850504591619886\n",
      "RMSE value on testing set = 4.813984903980081\n",
      "\n",
      "R = 0.001, lambda = 0.001\n",
      "alpha value = 22.961\n",
      "beta value  = 0.127,0.004\n",
      "time taken  = 0.9239199161529541s\n",
      "iterations  = 9192\n",
      "RMSE value on training set = 4.8504990401704\n",
      "RMSE value on testing set = 4.814000404673516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = standardize(train.iloc[:,0:30].values)\n",
    "y_train = train['MEDV'].values\n",
    "\n",
    "x_test = standardize(test.iloc[:,0:30].values)\n",
    "y_test = test['MEDV'].values\n",
    "\n",
    "# Initial beta values is an array of 2\n",
    "ini_alpha = -10\n",
    "ini_beta = np.zeros(30)+2\n",
    "\n",
    "# R = 0.0001, lambda = 1\n",
    "alpha1, beta1, total_time1 , iterations1 = ridge_multivariate_ols(x_train, y_train, ini_alpha, ini_beta, lamda = 1, R=0.0001, epsilon=0.0001, MaxIterations=10000)\n",
    "train_y_predict1 = alpha1\n",
    "test_y_predict1 = alpha1\n",
    "for i in range(0,30):\n",
    "    train_y_predict1 += x_train[:,i]*beta1[i]\n",
    "for i in range(0,30):\n",
    "    test_y_predict1 += x_test[:,i]*beta1[i]\n",
    "RMSE_train1 = compute_rmse(train_y_predict1, y_train)\n",
    "RMSE_test1 = compute_rmse(test_y_predict1, y_test)\n",
    "\n",
    "print('R = 0.0001, lambda = 1')\n",
    "print('alpha value = {:.3f}'.format(alpha1))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta1[0],beta1[1]))\n",
    "print('time taken  = {}s'.format(total_time1))\n",
    "print('iterations  = {}'.format(iterations1))\n",
    "print('RMSE value on training set = {}'.format(RMSE_train1))\n",
    "print('RMSE value on testing set = {}'.format(RMSE_test1))\n",
    "print('')\n",
    "\n",
    "# R = 0.0001, lambda = 0.1\n",
    "alpha2, beta2, total_time2 , iterations2 = ridge_multivariate_ols(x_train, y_train, ini_alpha, ini_beta,lamda = 0.1, R=0.0001, epsilon=0.0001, MaxIterations=10000)\n",
    "train_y_predict2 = alpha2\n",
    "test_y_predict2 = alpha2\n",
    "for i in range(0,30):\n",
    "    train_y_predict2 += x_train[:,i]*beta2[i]\n",
    "for i in range(0,30):\n",
    "    test_y_predict2 += x_test[:,i]*beta2[i]\n",
    "RMSE_train2 = compute_rmse(train_y_predict2, y_train)\n",
    "RMSE_test2 = compute_rmse(test_y_predict2, y_test)\n",
    "\n",
    "print('R = 0.0001, lambda = 0.1')\n",
    "print('alpha value = {:.3f}'.format(alpha2))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta2[0],beta2[1]))\n",
    "print('time taken  = {}s'.format(total_time2))\n",
    "print('iterations  = {}'.format(iterations2))\n",
    "print('RMSE value on training set = {}'.format(RMSE_train2))\n",
    "print('RMSE value on testing set = {}'.format(RMSE_test2))\n",
    "print('')\n",
    "\n",
    "# R = 0.001, lambda = 0.01\n",
    "alpha3, beta3, total_time3 , iterations3 = ridge_multivariate_ols(x_train, y_train, ini_alpha, ini_beta, lamda = 0.01, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "train_y_predict3 = alpha3\n",
    "test_y_predict3 = alpha3\n",
    "for i in range(0,30):\n",
    "    train_y_predict3 += x_train[:,i]*beta3[i]\n",
    "for i in range(0,30):\n",
    "    test_y_predict3 += x_test[:,i]*beta3[i]\n",
    "RMSE_train3 = compute_rmse(train_y_predict3, y_train)\n",
    "RMSE_test3 = compute_rmse(test_y_predict3, y_test)\n",
    "\n",
    "print('R = 0.001, lambda = 0.01')\n",
    "print('alpha value = {:.3f}'.format(alpha3))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta3[0],beta3[1]))\n",
    "print('time taken  = {}s'.format(total_time3))\n",
    "print('iterations  = {}'.format(iterations3))\n",
    "print('RMSE value on training set = {}'.format(RMSE_train3))\n",
    "print('RMSE value on testing set = {}'.format(RMSE_test3))\n",
    "print('')\n",
    "\n",
    "# R = 0.001, lambda = 0.001\n",
    "alpha4, beta4, total_time4 , iterations4 = ridge_multivariate_ols(x_train, y_train, ini_alpha, ini_beta, lamda = 0.001, R=0.001, epsilon=0.0001, MaxIterations=10000)\n",
    "train_y_predict4 = alpha4\n",
    "test_y_predict4 = alpha4\n",
    "for i in range(0,30):\n",
    "    train_y_predict4 += x_train[:,i]*beta4[i]\n",
    "for i in range(0,30):\n",
    "    test_y_predict4 += x_test[:,i]*beta4[i]\n",
    "RMSE_train4 = compute_rmse(train_y_predict4, y_train)\n",
    "RMSE_test4 = compute_rmse(test_y_predict4, y_test)\n",
    "\n",
    "print('R = 0.001, lambda = 0.001')\n",
    "print('alpha value = {:.3f}'.format(alpha4))\n",
    "print('beta value  = {:.3f},{:.3f}'.format(beta4[0],beta4[1]))\n",
    "print('time taken  = {}s'.format(total_time4))\n",
    "print('iterations  = {}'.format(iterations4))\n",
    "print('RMSE value on training set = {}'.format(RMSE_train4))\n",
    "print('RMSE value on testing set = {}'.format(RMSE_test4))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 4.3 above, the lowest RMSE value we got was with a lambda value of 0.001. However, the RMSE values for both lambda values of 0.01 and 0.001 are very very similar. They are 4.5256 for the training set, and 5.3066 for the testing set. These RMSE values are exactly the same as the RMSE values we got from 4.2. These values are less than the RMSE values for 3.2 which is 6.802, and lower than the RMSE value from the nearest neighbor problem set, which had a value of 10.21. It is interesting to note that when the lambda values were set high to 0.1 or 1, the RMSE values increase significantly to 13.19 on the training set, and 13.97 on the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit 2: Cross-validate lambda\n",
    "\n",
    "Use k-fold cross-validation to select the optimal value of lambda. Report the average RMSE across all training sets, and the average RMSE across all testing sets. How do these numbers compare to each other, to the RMSE from your previous efforts?  Finally, create a scatter plot that shows RMSE as a function of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extra Credit 3: AdaGrad\n",
    "\n",
    "AdaGrad is a method to implement gradient descent with different learning rates for each feature. Adaptive algorithms like this one are being extensively used especially in neural network training. Implement AdaGrad on 2.3 but now use CRIM, RM and DIS as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. Tune the algorithm until you estimate the regression coefficients within a tolerance of 1e-1. Use mini-batch gradient descent in this implementation. In summary for each parameter (in our case one intercept and three slopes) the update step of the gradient (in this example $\\beta_j$) at iteration $k$ of the GD algorithm becomes:\n",
    "\n",
    "$$\\beta_j=\\beta_j -\\frac{R}{\\sqrt{G^{(k)}_j}}\\frac{\\partial J(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$$ where\n",
    "$G^{(k)}_j=\\sum_{i=1}^{k} (\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j})^2$ and $R$ is your learning rate. The notation $\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$ corresponds to the value of the gradient at iteration $(i)$. Essentially we are \"storing\" information about previous iteration gradients. Doing that we effectively decrease the learning rate slower when a feature $x_i$ is sparse (i.e. has many zero values which would lead to zero gradients). Although this method is not necessary for our regression problem, it is good to be familiar with these methods as they are widely used in neural network training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
